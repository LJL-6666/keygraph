Dummy是什么？
Dummy是根据词语的上下文来计算出的词向量，Dummy的语义是由它周围的词决定的。这同时也是词向量的核心思想。

这里我们提出词向量的概念。
词向量又称"Word Embedding”，采用一种低维的向量表示方法来表示一个词，使得相似词的词向量距离相近，同时又能避免纬度过高的问题。
词向量实际上是一类技术，单个词在预定义的向量空间中被表示为实数向量，每个单词都映射到一个向量（用向量储存文本信息）。

自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些文字数学化。
缺点：任意两个词之间都是孤立的.光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。语料库大的时候纬度变得很高。

俩种训练模型
CBOW将一个词所在的上下文中的词作为输入，而那个词本身作为输出，也就是说，看到—一个上下文，希望大概能猜出这个词和它的意思。通过在一个大的语料库训练，得到—个从输入层到隐含层的权重模型。
2-gram比较常用
其中，wt=（0，…，1，…，0），是one-hot编码
以cBoW演示词向量生成过程
输入层：上下文单词的one-hot所有one-hot分别乘以共享的输入权重矩阵W所得的相加求平均作为隐层向量，size为1N.
乘以输出权重矩阵W"
得到向量{*激活函数处理得到-dim概率分布，概率最大的 lindex所指示的单词为预测出的中间词（target word）
与 true labe的one-hot做比较，误差越小越好


skip-gram它的做法是，将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词，2-gram比较常用。
其中，wt=（0，…，1，…，0），是one-hot编码


词向量举个例子，比如在一个文本中包含“猫”“狗”“爱凊”等若干单词，而这若干单词映射到向量空间中，猫”对应的向量为（0.10.20.3），“狗”对应的向量为（0.20.20.4），“爱情”对应的映射为（0.4-0.5-0.2），这个映射的过程就叫做词嵌入。
通过词嵌入这种方式将单词转变为词向量，机器便可对单词进行计算
通过计算不同词向量之间夹角余弦值 cosine而得出单词之间的相似性例如：V（“国王”）V（“男人“）+V“女人”）≈V（“女王”）
cos a< cos b则“猫与“狗更相似，与"爱情”差异较大

最终词向量的计算,用词语的one-hot编码乘以权重矩阵W即某个词的词向量就是矩阵W的某一行。

python计算词向量
gensim.models.word2vec Word2Vec（sentences，size=100，window=5，min count=5，sg=0）
sentences：一段分好词的文本
·size：词向量纬度
window：词向量计算的窗口，即上下文的长度
·min count.最小词频数
sg：词向量计算方式，O表示CBOW，1表示 Skip-garn
